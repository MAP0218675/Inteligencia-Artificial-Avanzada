{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, Imputer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = (SparkSession.builder \n",
    "    .appName(\"titanic_reto\") \n",
    "    .config(\"spark.sql.ansi.enabled\", \"false\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline \n",
    "\n",
    "1. Obtener las filas categóricas y numéricas\n",
    "2. Empleamos indexer y one hot encoding\n",
    "3. Estandarizamos los datos\n",
    "4. Entrenamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_titanic = spark.read.csv(\"train.csv\", header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| NULL|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| NULL|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| NULL|       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|NULL|    0|    0|          330877| 8.4583| NULL|       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|  E46|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075| NULL|       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333| NULL|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708| NULL|       C|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|   16.7|   G6|       S|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26.55| C103|       S|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8.05| NULL|       S|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082| 31.275| NULL|       S|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406| 7.8542| NULL|       S|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|   16.0| NULL|       S|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652| 29.125| NULL|       Q|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|NULL|    0|    0|          244373|   13.0| NULL|       S|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|   18.0| NULL|       S|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|NULL|    0|    0|            2649|  7.225| NULL|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "data_titanic.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_titanic.printSchema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PassengerId', 'int'),\n",
       " ('Survived', 'int'),\n",
       " ('Pclass', 'int'),\n",
       " ('Name', 'string'),\n",
       " ('Sex', 'string'),\n",
       " ('Age', 'double'),\n",
       " ('SibSp', 'int'),\n",
       " ('Parch', 'int'),\n",
       " ('Ticket', 'string'),\n",
       " ('Fare', 'double'),\n",
       " ('Cabin', 'string'),\n",
       " ('Embarked', 'string')]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_titanic.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_col = [column for column in [\"Sex\", \"Embarked\"] if column in data_titanic.columns]\n",
    "numeric_columns = [column for column, type in data_titanic.dtypes if type in [\"int\", \"double\"] and column not in [\"PassengerId\", \"Survived\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=f\"{c}_idx\", handleInvalid=\"keep\")\n",
    "    for c in cat_col\n",
    "]\n",
    "\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCols=[f\"{c}_idx\"], outputCols=[f\"{c}_oh\"], handleInvalid=\"keep\")\n",
    "    for c in cat_col\n",
    "]\n",
    "\n",
    "imputer = Imputer(inputCols=numeric_columns, outputCols=[f\"{c}_imp\" for c in numeric_columns])\n",
    "feature_cols = [f\"{c}_imp\" for c in numeric_columns] + [f\"{c}_oh\" for c in cat_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\", handleInvalid=\"keep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withMean=False, withStd=True)\n",
    "\n",
    "svc = LinearSVC(\n",
    "    featuresCol=\"scaledFeatures\",\n",
    "    labelCol=\"label\",\n",
    "    maxIter=100,\n",
    "    regParam=0.05,\n",
    "    standardization=False \n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[*indexers, *encoders, imputer, assembler, scaler, svc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_data = data_titanic[numeric_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_titanic.withColumn(\"label\", F.col(\"Survived\").cast(\"double\"))\n",
    "train, test = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "model = pipeline.fit(train)\n",
    "pred  = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+----------+----------------------------------------+\n",
      "|PassengerId|label|prediction|rawPrediction                           |\n",
      "+-----------+-----+----------+----------------------------------------+\n",
      "|3          |1.0  |1.0       |[-0.9992630453943617,0.9992630453943617]|\n",
      "|7          |0.0  |0.0       |[0.9918137785619028,-0.9918137785619028]|\n",
      "|9          |1.0  |1.0       |[-0.9783133639971993,0.9783133639971993]|\n",
      "|14         |0.0  |0.0       |[1.0841668235403317,-1.0841668235403317]|\n",
      "|20         |1.0  |1.0       |[-1.004509508527744,1.004509508527744]  |\n",
      "|24         |1.0  |0.0       |[0.9844591532800553,-0.9844591532800553]|\n",
      "|30         |0.0  |0.0       |[1.0079574967408202,-1.0079574967408202]|\n",
      "|36         |0.0  |0.0       |[1.0116708582634706,-1.0116708582634706]|\n",
      "|46         |0.0  |0.0       |[1.0078956410651276,-1.0078956410651276]|\n",
      "|47         |0.0  |0.0       |[1.029179001647377,-1.029179001647377]  |\n",
      "+-----------+-----+----------+----------------------------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "pred.select(\"PassengerId\",\"label\",\"prediction\",\"rawPrediction\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7793 | F1: 0.7771\n"
     ]
    }
   ],
   "source": [
    "e_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "e_f1  = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "acc = e_acc.evaluate(pred)\n",
    "f1  = e_f1.evaluate(pred)\n",
    "\n",
    "print(f\"Accuracy: {acc:.4f} | F1: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/05 19:11:54 WARN DAGScheduler: Broadcasting large task binary with size 1295.8 KiB\n",
      "25/09/05 19:11:54 WARN DAGScheduler: Broadcasting large task binary with size 1770.0 KiB\n",
      "25/09/05 19:11:55 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "25/09/05 19:11:58 WARN DAGScheduler: Broadcasting large task binary with size 1501.5 KiB\n",
      "25/09/05 19:11:58 WARN DAGScheduler: Broadcasting large task binary with size 1513.3 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC (valid): 0.9187\n",
      "Accuracy (valid): 0.8345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/05 19:11:59 WARN DAGScheduler: Broadcasting large task binary with size 1482.9 KiB\n",
      "25/09/05 19:11:59 WARN DAGScheduler: Broadcasting large task binary with size 1490.7 KiB\n",
      "25/09/05 19:11:59 WARN DAGScheduler: Broadcasting large task binary with size 1706.2 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo de envío guardado como: submission_rf.csv\n",
      "\n",
      "Importancias (resumen):\n",
      " - Pclass: importancia aproximada disponible en el vector (ver OHE aparte)\n",
      " - Age_imp: importancia aproximada disponible en el vector (ver OHE aparte)\n",
      " - SibSp: importancia aproximada disponible en el vector (ver OHE aparte)\n",
      " - Parch: importancia aproximada disponible en el vector (ver OHE aparte)\n",
      " - Fare_imp: importancia aproximada disponible en el vector (ver OHE aparte)\n",
      " - FamilySize: importancia aproximada disponible en el vector (ver OHE aparte)\n",
      " - IsAlone: importancia aproximada disponible en el vector (ver OHE aparte)\n",
      " - Variables OHE (Sex, Embarked, Title) se distribuyen en múltiples posiciones del vector.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, Imputer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Titanic-RF-SparkML\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "TRAIN_PATH = \"train.csv\"\n",
    "TEST_PATH  = \"test.csv\"\n",
    "\n",
    "train = spark.read.csv(TRAIN_PATH, header=True, inferSchema=True)\n",
    "test  = spark.read.csv(TEST_PATH,  header=True, inferSchema=True)\n",
    "\n",
    "train = train.withColumn(\"Survived\", F.col(\"Survived\").cast(\"double\"))\n",
    "test  = test.withColumn(\"Survived\", F.lit(None).cast(\"double\"))\n",
    "\n",
    "full_df = train.unionByName(test, allowMissingColumns=True)\n",
    "\n",
    "title_regex = F.regexp_extract(F.col(\"Name\"), r\",\\s*([^\\.]+)\\.\", 1)\n",
    "full_df = full_df.withColumn(\"TitleRaw\", F.trim(title_regex))\n",
    "\n",
    "rare_titles = [\"Lady\",\"Countess\",\"Capt\",\"Col\",\"Don\",\"Dr\",\"Major\",\"Rev\",\"Sir\",\"Jonkheer\",\"Dona\"]\n",
    "full_df = full_df.withColumn(\n",
    "    \"Title\",\n",
    "    F.when(F.col(\"TitleRaw\").isin(\"Mlle\",\"Ms\"), \"Miss\")\n",
    "     .when(F.col(\"TitleRaw\") == \"Mme\", \"Mrs\")\n",
    "     .when(F.col(\"TitleRaw\").isin(rare_titles), \"Rare\")\n",
    "     .otherwise(F.col(\"TitleRaw\"))\n",
    ")\n",
    "\n",
    "full_df = full_df.withColumn(\"FamilySize\", (F.col(\"SibSp\") + F.col(\"Parch\") + F.lit(1)).cast(\"double\"))\n",
    "full_df = full_df.withColumn(\"IsAlone\", F.when(F.col(\"FamilySize\") == 1, 1.0).otherwise(0.0))\n",
    "\n",
    "numeric_cols = [\"Age\", \"Fare\", \"Pclass\", \"SibSp\", \"Parch\", \"FamilySize\", \"IsAlone\"]\n",
    "full_df = full_df.withColumn(\"Pclass\", F.col(\"Pclass\").cast(\"double\"))\n",
    "\n",
    "imputer = Imputer(\n",
    "    strategy=\"median\",\n",
    "    inputCols=[\"Age\", \"Fare\"],\n",
    "    outputCols=[\"Age_imp\", \"Fare_imp\"]\n",
    ")\n",
    "\n",
    "cat_cols = [\"Sex\", \"Embarked\", \"Title\"]\n",
    "\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=f\"{c}_idx\", handleInvalid=\"keep\")\n",
    "    for c in cat_cols\n",
    "]\n",
    "\n",
    "encoder = OneHotEncoder(\n",
    "    inputCols=[f\"{c}_idx\" for c in cat_cols],\n",
    "    outputCols=[f\"{c}_oh\" for c in cat_cols],\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "feature_cols = [\n",
    "    \"Pclass\",\n",
    "    \"Age_imp\",\n",
    "    \"SibSp\",\n",
    "    \"Parch\",\n",
    "    \"Fare_imp\",\n",
    "    \"FamilySize\",\n",
    "    \"IsAlone\",\n",
    "    \"Sex_oh\",\n",
    "    \"Embarked_oh\",\n",
    "    \"Title_oh\"\n",
    "]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    labelCol=\"Survived\",\n",
    "    featuresCol=\"features\",\n",
    "    numTrees=200,\n",
    "    maxDepth=8,\n",
    "    minInstancesPerNode=2,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "stages = [imputer] + indexers + [encoder, assembler, rf]\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "train_df = full_df.filter(F.col(\"Survived\").isNotNull())\n",
    "test_df  = full_df.filter(F.col(\"Survived\").isNull())\n",
    "\n",
    "train_split, valid_split = train_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "model = pipeline.fit(train_split)\n",
    "\n",
    "valid_pred = model.transform(valid_split)\n",
    "\n",
    "bin_eval = BinaryClassificationEvaluator(\n",
    "    labelCol=\"Survived\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "auc = bin_eval.evaluate(valid_pred)\n",
    "\n",
    "acc_eval = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"Survived\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "acc = acc_eval.evaluate(valid_pred)\n",
    "\n",
    "print(f\"AUC (valid): {auc:.4f}\")\n",
    "print(f\"Accuracy (valid): {acc:.4f}\")\n",
    "\n",
    "test_pred = model.transform(test_df)\n",
    "\n",
    "submission = (\n",
    "    test_pred\n",
    "    .select(\"PassengerId\", F.col(\"prediction\").cast(T.IntegerType()).alias(\"Survived\"))\n",
    "    .orderBy(\"PassengerId\")\n",
    ")\n",
    "\n",
    "output_path = \"submission_rf.csv\"\n",
    "(\n",
    "    submission\n",
    "    .coalesce(1)\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"header\", True)\n",
    "    .csv(\"submission_rf_tmp\")\n",
    ")\n",
    "\n",
    "# Renombrar el único part file a submission_rf.csv desde Python \"normal\"\n",
    "# si estás en un entorno donde puedas usar utilidades del sistema:\n",
    "import os, shutil\n",
    "for f in os.listdir(\"submission_rf_tmp\"):\n",
    "    if f.startswith(\"part-\") and f.endswith(\".csv\"):\n",
    "        shutil.move(os.path.join(\"submission_rf_tmp\", f), output_path)\n",
    "        break\n",
    "shutil.rmtree(\"submission_rf_tmp\")\n",
    "\n",
    "print(\"Archivo de envío guardado como:\", output_path)\n",
    "\n",
    "rf_model = model.stages[-1]  # RandomForestClassifierModel\n",
    "importances = rf_model.featureImportances\n",
    "\n",
    "# Para mapear importancias a columnas, necesitamos expandir las OH:\n",
    "# Calculamos el tamaño de cada OHE para construir nombres expandibles\n",
    "# (esto es aproximado; si quieres exactitud, inspecciona los attrs del encoder).\n",
    "# Aquí mostramos un enfoque simple: reporta importancias sumando por grupo.\n",
    "base_cols = [\"Pclass\",\"Age_imp\",\"SibSp\",\"Parch\",\"Fare_imp\",\"FamilySize\",\"IsAlone\"]\n",
    "oh_groups = [\"Sex_oh\", \"Embarked_oh\", \"Title_oh\"]\n",
    "\n",
    "# Como resumen: imprimimos importancias de las columnas base (primeras posiciones)\n",
    "# y dejamos una nota de que OHE se divide en varias columnas internas.\n",
    "print(\"\\nImportancias (resumen):\")\n",
    "for i, col in enumerate(base_cols):\n",
    "    # orden aproximado según VectorAssembler: base_cols primero\n",
    "    print(f\" - {col}: importancia aproximada disponible en el vector (ver OHE aparte)\")\n",
    "print(\" - Variables OHE (Sex, Embarked, Title) se distribuyen en múltiples posiciones del vector.\\n\")\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
